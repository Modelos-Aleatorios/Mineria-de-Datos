---
title: "Proyecto Tópicos I - Outliers"
author: "Alexander A. Ramírez M. (alexanderramirez.me) y Daysi Febles (daysilorenafeblesr@gmail.com)"
output:
  pdf_document: 
    toc: yes
    toc_depth: 5
    fig_width: 6
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("logspline")
library(MASS)
library(readr)
library(ggplot2)
library(fitdistrplus)
library(logspline)
library(ExtDist)
```

----

# Outliers 

Los outliers son observaciones que se alejan a la gran mayoria de los datos que se tienen. Al aplicar pruebas estadísticas éstos valores tiendes a sesgar los resultados, y llevar a conclusiones no acertadas sobre la realidad de los datos, por ejemplo si se quiere estimar la distribución de los datos puede que estos valores nos alejen de la verdadera distribución dándonos otra distribución no apropiada para explicar mejor los datos.

Trabajaremos con los datos donde la variable `Impr` es mayor a cero.

```{r, echo=FALSE,warning=FALSE,message=FALSE}
data <- read_delim("../data/ROW.csv", 
    ";", escape_double = FALSE, locale = locale(decimal_mark = ",", 
    grouping_mark = "."), trim_ws = TRUE)
#Eliminada la variable Allconv
data<-data[,-13]
#Renombrar las variables
colnames(data)<-c("Adgroup","MaxCPC","Clicks","Impr",
                    "CTR","AvgCPC","Costo","AvgPos","Qscore",
                    "Ventas","Costc","Convr")
data_original<-data
#Eliminar los datos que tienen 0 impresiones
data1<-data[,-1]
data2<-data1[data1$Impr!=0,]
```

Para encontrar estos datos atípicos o poco comunes, se usan pruebas como la Chi-Cuadrado, entre otras, el problema de estas pruebas es que se tiene que asumir normalidad de los datos, en nuestro caso no parece cumplirse con esta suposición, veamos si es así, primero comprobaremos si cada variable se distribuye normalmente:

```{r}
#Variable MaxCPC
shapiro.test(data2$MaxCPC)
#Variable Clicks
shapiro.test(data2$Clicks)
#Variable Impr
shapiro.test(data2$Impr)
#Variable CTR
shapiro.test(data2$CTR)
#Variable AvgCPC
shapiro.test(data2$AvgCPC)
#Variable Costo
shapiro.test(data2$Costo)
#Variable AvgPos
shapiro.test(data2$AvgPos)
#Variable Qscore
shapiro.test(data2$Qscore)
#Variable Ventas
shapiro.test(data2$Ventas)
#Variable Costc
shapiro.test(data2$Costc)
#Variable Convr
shapiro.test(data2$Convr)
```

La prueba que acabamos de aplicar es la prueba de Shapiro, contrasta la hipótesis nula de que los datos se distribuyen normales, contra que no lo son; en todos los resultados el p-valor es menor que el nivel de significancia $\alpha=0.05$, por lo tanto en todos los casos se rechaza la hipótesis nula, es decir, ninguna de las variables se distribuye normalmente. Ya con esto en mano probaremos con otras funciones conocidas. 

Para el cálculo de los outlier utilizaremos los diagrama de cajas, éstos nos dan la mediana de los datos y los quantiles 1, 2 y 3 los valores que queden fuera de el tercer cuantil y primer cuantil son considerados datos atípicos, también con este gráfico podemos ver la asimetría de la distribución. Veamos que forma tienen los diagramas de cajas para todas las variables:

```{r}
boxplot(data2, main="Diagrama de cajas",xlab="Variables")

#Función para eliminar los outliers segun el cuantil 1 y 3
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}
```

Por lo que observamos los diagramas a simple vista no nos dicen mucho, solo que estan muy cercanos a cero.

Realizaremos el estudio para cada una de las variables para ver que sucede en cada caso con más detalle.




A continuación vamos ajustar los datos a otras funciones de densidad conocidas distintas a la distribución Normal, estas comparaciones las haremos con los datos de las variable completos y sin los outliers, para sacar los outiler utilizaremos la función anterior que utiliza la suposición de outlier en los diagramas de cajas. 

Estimaremos los parámetros de las funciones supuestas con el comando `fitdistr()` y `fitdist()`, estas funciones me dan los mismos valores, solo que hay funciones que no estan incluidas en ambos. Luego de la estimación de los parámetros se aplicara una prueba de Kolmogórov smirnov y un test de Wilcoxon para comprobar que los datos se distribuyen como la función encontrada.

----

## a) Variable **MaxCPC**

Las funciones de densidad que utilizaremos para esta variable son Exponencial, Gamma y Weibull, ya que ella toma valores continuos y positivos.

```{r}
dat<-data2$MaxCPC
summary(dat)
par(mfrow=c(2,2))
plot(dat)
boxplot(dat)
hist(dat)
plot(density(dat))

p<-as.vector(fitdistr(dat,"exponential")$estimate)
fitexp <- fitdist(dat,"exp")
plot(fitexp)

#Prueba de Kolmogórov-smirnov y Test de Wilconxo
ks.test(dat,"pexp",rate=p)
wilcox.test(dat,rexp(length(dat),rate=p),paired = FALSE)
```

Aunque se observa un buen ajuste de la función de densidad al histograma de los datos y con la función de distribución, cuando se realizan las pruebas correspondientes para contrastar si los datos provienen o no de una distribución Exponencial en ambos casos se rechaza la hipótesis nula, entonces podemos decir que los datos asociados a esta variable no se distribuyen Exponencial. Veamos que pasa con la aproximación de una función de densidad Gamma:

```{r}
p<-as.vector(fitdistr(dat,"gamma")$estimate)
fitgam<-fitdist(dat,"gamma")
plot(fitgam)
ks.test(dat, "pgamma",shape=p[1],rate=p[2])
wilcox.test(dat, rgamma(length(dat),shape=p[1],rate=p[2]) , paired = FALSE)
```

En este caso se observa mejor ajuste en el histograma y la distribución de los datos, aunque se rechaza la hipótesis nula en la prueba de Kolmogórov se acepta para el caso de la prueba de Wilcoxon, entonces podemos decir que estos datos se distribuyen Gamma. Veamos finalmente que pasa si la aproximamos con una densidad Weibull:

```{r,warning=FALSE}
p<-as.vector(fitdistr(dat,"weibull")$estimate)
fitwei<-fitdist(dat, "weibull")
plot(fitwei)
ks.test(dat, "pweibull",shape=p[1], scale=p[2])
wilcox.test(dat, rweibull(length(dat),shape=p[1], scale=p[2]) , paired = FALSE)
```

Sucede lo mismo que la aproximación a una Gamma, se rechaza la hipótesis nula de Komogorov pero se acepta en el caso de la prueba de Wilcoxon. Entonces podríamos decir que estos datos se distribuyen Weibull o Gamma bajo la prueba de Wilcoxon. Para seleccionar una de estas dos distribuciones veremos los valores correspondientes al criterio de Akaike:

```{r}
fitwei$aic
fitgam$aic
```

Tomando el valor mínimo entre estos dos, nos quedamos con la distribución Gamma.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
p<-as.vector(fitdistr(dat,"gamma")$estimate)
p
datos<-data.frame(dat,
       wei=dgamma(dat,shape=p[1],rate=p[2]))

ggplot(data=datos, mapping = aes(dat))+
geom_histogram(aes(y =..density..),col=I("blue"), alpha=I(.2))+
geom_line(data=datos, aes(dat,wei,colour = I("red")))+
labs(title ="Variable MaxCPC", x = "x", y = "f(x)")+
theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

Veamos que sucede con estos datos sin Outliers:

```{r, warning=FALSE}
#Removiendo los outliers, para comparar distribuciones
y <- na.omit(remove_outliers(dat))
dat<-as.numeric(y)

#Aproximarlo con una Exponencial
p<-as.vector(fitdistr(dat,"exponential")$estimate)
fitexp <- fitdist(dat,"exp")
#plot(fitexp)
ks.test(dat,"pexp",rate=p)
wilcox.test(dat,rexp(length(dat),rate=p),paired = FALSE)

#Aproximarlo con una Gamma
p<-as.vector(fitdistr(dat,"gamma")$estimate)
fitgam<-fitdist(dat,"gamma")
#plot(fitgam)
ks.test(dat, "pgamma",shape=p[1],rate=p[2])
wilcox.test(dat, rgamma(length(dat),shape=p[1],rate=p[2]) , paired = FALSE)

#Aproximarlo con una Weibull
p<-as.vector(fitdistr(dat,"weibull")$estimate)
fitwei<-fitdist(dat, "weibull")
#plot(fitwei)
ks.test(dat, "pweibull",shape=p[1], scale=p[2])
wilcox.test(dat, rweibull(length(dat),shape=p[1], scale=p[2]) , paired = FALSE)
```

Encontramos los mismos resultados que para los datos completos, entonces podemos decir que los datos se distribuyen Gamma o Weibull. Ahora calcularemos los correspondientes valores de AIC,

```{r,warning=FALSE}
fitwei$aic
fitgam$aic
```

Seleccionando el menor de estos valores nos quedamos con la distribución Weibull.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
p
datos<-data.frame(y,
       wei=dweibull(dat,shape=p[1], scale=p[2]))

ggplot(data=datos, mapping = aes(y))+geom_histogram(aes(y =..density..),fill=I("blue"),col=I("blue"), alpha=I(.2))+
  geom_line(data=datos, aes(y,wei,colour = I("red")))+
  labs(title ="Variable MaxCPC", x = "x", y = "f(x)")+
  theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

```{r}
boxplot(y)
```

Mediante el diagrama de caja sin los outliers vemos un comportamiento poco asimétrico, con más peso sobre los valores mayores a la mediana, y esto es lo que se observa con las distribución aproximada. En el diagrama de caja de los datos completos no estaba muy claro el comportamiento de los datos, solo se observaba que estaban muy próximos a cero.

----

## b) Variable **Clicks**

Esta variable toma valores discretos positivos por lo tanto la vamos aproximar a una distribución Poisson, Geométrica y Binomial negativa.

```{r}
dat<-data2$Clicks
summary(dat)
par(mfrow=c(2,2))
plot(dat)
boxplot(dat)
hist(dat)
plot(density(dat))

p<-as.vector(fitdistr(dat,"Poisson")$estimate)
fitpois<-fitdist(dat,"pois")
plot(fitpois)
ks.test(dat,"ppois",lambda=p)
wilcox.test(dat,rpois(length(dat),lambda =p),paired=FALSE)
```

En ambas pruebas se rechaza la hipotesis nula, es decir la variable `Clicks` no se distribuye como una Poisson. 

Ahora probaremos con una distribución Geométrica:

```{r}
p<-as.vector(fitdistr(dat,"geometric")$estimate)
fitgeom <- fitdist(dat, "geom")
plot(fitgeom)
ks.test(dat, "pgeom",prob=p)
set.seed(111)
wilcox.test(dat , rgeom(length(dat),prob=p) , paired = FALSE)
```

También se rechaza la hipótesis nula en ambas pruebas, probemos finalmente con una distribución binomial negativa:
```{r,warning=FALSE}
p<-as.vector(fitdistr(dat, "negative binomial")$estimate)
fitnbinom <- fitdist(dat, "nbinom")
plot(fitnbinom)
ks.test(dat,"pnbinom",size=p[1],mu=p[2])
wilcox.test(dat,rnbinom(length(dat),size=p[1],mu=p[2]), paired = FALSE)
```

En este caso se rechaza la prueba de Kolmogorov, pero se acepta la hipótesis nula de la prueba de Wilcoxon, por lo tanto podemos decir que la variable aleatoria `Clicks` se distribuye Binomial negativa.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
p
datos<-data.frame(dat,nbinom=dnbinom(dat,size=p[1],mu=p[2]))

ggplot(data=datos, mapping = aes(dat,nbinom, colour = I("red")))+geom_point()+
geom_histogram(mapping = aes(dat,y =..density..,col = I("blue")),fill=I("blue"),alpha=I(.1))+
  labs(title ="Variable Clicks", x = "x", y = "f(x)")+
  ylim(0,0.04)+
  theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

Veamos que pasa con los datos sin los outliers:

```{r,warning=FALSE}
y <- na.omit(remove_outliers(dat))
dat<-as.numeric(y)

p<-as.vector(fitdistr(dat,"Poisson")$estimate)
fitpois<-fitdist(dat,"pois")
#plot(fitpois)
ks.test(dat,"ppois",lambda=p)
wilcox.test(dat,rpois(length(dat),lambda =p),paired=FALSE)

p<-as.vector(fitdistr(dat,"geometric")$estimate)
fitgeom <- fitdist(dat, "geom")
#plot(fitgeom)
ks.test(dat, "pgeom",prob=p)
wilcox.test(dat , rgeom(length(dat),prob=p) , paired = FALSE)

p<-as.vector(fitdistr(dat, "negative binomial")$estimate)
fitnbinom <- fitdist(dat, "nbinom")
#plot(fitnbinom)
ks.test(dat,"pnbinom",size=p[1],mu=p[2])
wilcox.test(dat,rnbinom(length(dat),size=p[1],mu=p[2]), paired = FALSE)
```

Por lo observado solo se acepta la hipótesis nula en la prueba de Wilcoxon para la comparación de la binomial negativa para los datos sin los outliers. Recordemos que este fue el mismo resultado para los datos completos.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
p
datos<-data.frame(y, nbinom=dnbinom(y,size = p[1], mu = p[2]))

ggplot(data=datos, mapping = aes(y,nbinom, colour = I("red")))+geom_point()+
  geom_histogram(mapping = aes(y,y =..density..,col = I("blue")),fill=I("blue"),alpha=I(.1))+
  labs(title ="Variable Clicks", x = "x", y = "f(x)")+
  theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

```{r}
boxplot(y)
```

En el `boxplot` vemos que siguen existiendo datos atípicos y que la mayor concentración de datos está cerca de cero, y este comportamiento corresponde con la distribución Binomial negativa aproximada.

----

## c) Variable **Impr**

Esta variable al igual que la anterior es discreta y toma valores positivos, la aproximaremos igual con una distribución Poisson, Geométrica y Binomial negativa.

```{r}
dat<-data2$Impr
summary(dat)
par(mfrow=c(2,2))
plot(dat)
boxplot(dat)
hist(dat)
plot(density(dat))

p<-as.vector(fitdistr(dat,"Poisson")$estimate)
fitpois<-fitdist(dat,"pois")
plot(fitpois)
ks.test(dat,"ppois",lambda=p)
wilcox.test(dat,rpois(length(dat),lambda =p),paired=FALSE)
```

En ambas pruebas se rechaza la hipotesis nula, es decir esta variable no se distribuye como una Poisson. Ahora probaremos con una distribución Geométrica:

```{r}
p<-as.vector(fitdistr(dat,"geometric")$estimate)
#fitexp<-fitdist(dat,"exp") #ESTE COMANDO AQUI ME DA PROBLEMA
#plot(fitexp)
ks.test(dat, "pgeom",prob=p)
wilcox.test(dat , rgeom(length(dat),prob=p) , paired = FALSE)
```

También se rechaza la hipótesis nula en ambas pruebas, probemos con una distribución Binomial negativa:

```{r}
p<-as.vector(fitdistr(dat, "negative binomial")$estimate)
fitnbinom <- fitdist(dat, "nbinom")
plot(fitnbinom)
ks.test(dat,"pnbinom",size=p[1],mu=p[2])
wilcox.test(dat,rnbinom(length(dat),size=p[1],mu=p[2]), paired = FALSE)
```

En todas las distribuciones se rechaza la hipótesis nula en ambas pruebas, por lo tanto no podemos afirmar si estos datos se distribuyen Poisson, Geométrica o Binomial Negativa.

Veamos que pasa con los datos sin outliers, compararemos con las mismas distribuciones.

```{r}
y <- na.omit(remove_outliers(dat))
dat<-as.numeric(y)

p<-as.vector(fitdistr(dat,"Poisson")$estimate)
fitpois<-fitdist(dat,"pois")
#plot(fitpois)
ks.test(dat,"ppois",lambda=p)
wilcox.test(dat,rpois(length(dat),lambda =p),paired=FALSE)

p<-as.vector(fitdistr(dat,"geometric")$estimate)
#fitgeom <- fitdist(dat, "geom") #ESTE COMANDO ME DA PROBLEMAS
#plot(fitgeom)
ks.test(dat, "pgeom",prob=p)
wilcox.test(dat , rgeom(length(dat),prob=p) , paired = FALSE)

p<-as.vector(fitdistr(dat, "negative binomial")$estimate)
fitnbinom <- fitdist(dat, "nbinom")
#plot(fitnbinom)
ks.test(dat,"pnbinom",size=p[1],mu=p[2])
wilcox.test(dat,rnbinom(length(dat),size=p[1],mu=p[2]), paired = FALSE)
```

Se acepta la prueba de Wilcoxon para el caso por lo tanto podemos decir que estos datos se distribuyen de acuerdo a la distribución Binomial negativa.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
p
datos<-data.frame(y, nbinom=dnbinom(y,size = p[1], mu = p[2]))

ggplot(data=datos, mapping = aes(y,nbinom, colour = I("red")))+geom_point()+
  geom_histogram(mapping = aes(y,y =..density..,col = I("blue")),fill=I("blue"),alpha=I(.1))+
  labs(title ="Variable Impr", x = "x", y = "f(x)")+
  theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

```{r}
boxplot(y)
```

----

## d) Variable **CTR**

Esta variable toma valores en el intervalo $[0,1]$, por lo tanto la aproximaremos a una distribución Uniforme en $[0,1]$ y una Beta estandar.

```{r,warning=FALSE}
dat<-data2$CTR
summary(dat)
par(mfrow=c(2,2))
plot(dat)
boxplot(dat)
hist(dat)
plot(density(dat))

#Aproximación Uniforme en [0,1]
ks.test(dat, "punif",min=0,max=1)
wilcox.test(dat,runif(length(dat),min=0,max=1),paired = FALSE)

#Aproximando una Uniforme
funu<-eUniform(dat)
plot(funu)
ks.test(dat, "punif",min=funu$a,max=funu$b)
wilcox.test(dat,runif(length(dat),min=funu$a,max=funu$b),paired = FALSE)

#Aproximación Beta estandar
funb<-eBeta(dat)
plot(funb)
ks.test(dat, "pbeta",shape1=funb$shape1,shape2=funb$shape2)
wilcox.test(dat,rbeta(length(dat),shape1=funb$shape1,shape2=funb$shape2),paired = FALSE)
```

Para el caso de la distribución uniforme se rechaza la hipótesis de que los datos se distribuyen de esa manera, luego para la densidad Beta se acepta la hipótesis nula que los datos siguen esta distribución con la prueba de Wilcoxon, por lo tanto los aproximaremos como una Beta. 

```{r,echo=FALSE,message=FALSE,warning=FALSE}
funb
datos<-data.frame(dat,
                  beta=dbeta(dat,shape1=funb$shape1,shape2=funb$shape2))

ggplot(data=datos, mapping = aes(dat,beta, colour = I("red")))+geom_line()+
geom_histogram(mapping = aes(dat,y =..density..,col = I("blue")),fill=I("blue"),alpha=I(.1))+
  labs(title ="Variable CTR", x = "x", y = "f(x)")+
  ylim(0,12)+
  theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

Veamos que pasa si sacamos los outliers:

```{r,warning=FALSE, message=FALSE}
y <- na.omit(remove_outliers(dat))
dat<-y

ks.test(dat, "punif",min=0,max=1)
wilcox.test(dat,runif(length(dat),min=0,max=1),paired = FALSE)

funu<-eUniform(dat)
#plot(funu)
ks.test(dat, "punif",min=funu$a,max=funu$b)
wilcox.test(dat,runif(length(dat),min=funu$a,max=funu$b),paired = FALSE)

funb<-eBeta(dat)
#plot(funb)
ks.test(dat, "pbeta",shape1=funb$shape1,shape2=funb$shape2)
wilcox.test(dat,rbeta(length(dat),shape1=funb$shape1,shape2=funb$shape2),paired = FALSE)
```

Notemos que los límites del intervalo de confianza cambia, este es más pequeño que de los datos originales, pero a pesar de esto, en todos los casos se rechaza la hipótesis nula con ambas pruebas.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
funu
datos<-data.frame(y,
                  unif=dunif(y,min=funu$a,max=funu$b),
                  beta=dbeta(y,shape1=funb$shape1,shape2=funb$shape2))

ggplot(data=datos, mapping = aes(y,unif, colour = I("green")))+geom_line()+
geom_line( aes(y,beta, colour = I("red"))) + 
geom_histogram(mapping = aes(y,y =..density..,col = I("blue")),fill=I("blue"),alpha=I(.1))+
labs(title ="Variable CTR", x = "x", y = "f(x)")+
theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

```{r}
boxplot(y)
```

Por los gráficos observados y los resultados obtenidos en las pruebas realizadas concluimos que la distribución Beta es la que mejor se ajusta a esta variable.

----

## e) Variable **AvgCPC**

Al igual que a la variable anterior pensamos que esta variable podría ser Uniforme o Beta en el intervalo $[0,1]$, veamos si es así:

```{r,warning=FALSE}
dat<-data2$AvgCPC
summary(dat)
par(mfrow=c(2,2))
plot(dat)
boxplot(dat)
hist(dat)
plot(density(dat))

ks.test(dat, "punif",min=0,max=1)
wilcox.test(dat,runif(length(dat),min=0,max=1),paired = FALSE)

funu<-eUniform(dat)
plot(funu)
ks.test(dat, "punif",min=funu$a,max=funu$b)
wilcox.test(dat,runif(length(dat),min=funu$a,max=funu$b),paired = FALSE)

funb<-eBeta(dat)
plot(funb)
ks.test(dat, "pbeta",shape1=funb$shape1,shape2=funb$shape2)
wilcox.test(dat,rbeta(length(dat),shape1=funb$shape1,shape2=funb$shape2),paired = FALSE)
```

En todas las pruebas se rechazo la hipótesis nula. Veamos que obtenemos cuando sacamos los outliers de estos datos:

```{r,warning=FALSE}
y <- na.omit(remove_outliers(dat))
dat<-y

ks.test(dat, "punif",min=0,max=1)
wilcox.test(dat,runif(length(dat),min=0,max=1),paired = FALSE)

funu<-eUniform(dat)
#plot(funu)
ks.test(dat, "punif",min=funu$a,max=funu$b)
wilcox.test(dat,runif(length(dat),min=funu$a,max=funu$b),paired = FALSE)

funb<-eBeta(dat)
#plot(funb)
ks.test(dat, "pbeta",shape1=funb$shape1,shape2=funb$shape2)
wilcox.test(dat,rbeta(length(dat),shape1=funb$shape1,shape2=funb$shape2),paired = FALSE)
```

En todos los casos se rechazo la hipótesis nula, dibujemos todas las funciones estimadas para ver cual se aproxima mejor al histograma de los datos.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
funu
funb
datos<-data.frame(y,
                  uni1=dunif(y,min=0,max=1),
                  uni=dunif(y,min=funu$a,max=funu$b),
                  beta=dbeta(y,shape1=funb$shape1,shape2=funb$shape2))

ggplot(data=datos, mapping = aes(y,beta, colour = I("red")))+geom_line()+
geom_line(aes(y,uni1, colour=I("green")))+
geom_line(aes(y,uni, colour=I("black")))+  
geom_histogram(mapping = aes(y,y =..density..,col = I("blue")),fill=I("blue"),alpha=I(.1))+
  labs(title ="Variable AvgCPC", x = "x", y = "f(x)")+
  theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))

boxplot(y)
```

Con el gráfico del Boxplot vemos que los datos estan concentrados hacia del cero, dicho comportamiento concuerda con la función Beta estimada. Así concluimos que estos datos corresponden a una función de distribución Beta.

----

## f) Variable **Costo**

Las funciones de densidad que utilizaremos para esta variable son Exponencial, Gamma y Weibull, ya que ella toma valores continuos y positivos.

```{r}
dat<-data2$Costo
summary(dat)
par(mfrow=c(2,2))
plot(dat)
boxplot(dat)
hist(dat)
plot(density(dat))

p<-as.vector(fitdistr(dat,"exponential")$estimate)
fitexp <- fitdist(dat,"exp")
plot(fitexp)
ks.test(dat,"pexp",rate=p)
wilcox.test(dat,rexp(length(dat),rate=p),paired = FALSE)
```

Observando los resultados de las pruebas, en ambos casos rechazamos la hipótesis nula, descartando así esta distribución, veamos con la distribución Gamma:

```{r}
p<-eGamma(dat)
plot(p)
ks.test(dat, "pgamma",shape=as.numeric(p[1]),rate=as.numeric(p[2]))
wilcox.test(dat, rgamma(length(dat),shape=as.numeric(p[1]),
                        rate=as.numeric(p[2])),paired = FALSE)
```

De igual manera que en el caso de la función de densidad Exponencial se rechaza la hipótesis nula en ambas pruebas.

Veamos que pasa con los datos sin outliers:

```{r,warning=FALSE}
y <- na.omit(remove_outliers(dat))
dat<-as.numeric(y)

p<-as.vector(fitdistr(dat,"exponential")$estimate)
fitexp <- fitdist(dat,"exp")
#plot(fitexp)
ks.test(dat,"pexp",rate=p)
wilcox.test(dat,rexp(length(dat),rate=p),paired = FALSE)

p<-eGamma(dat)
#plot(p)
ks.test(dat, "pgamma",shape=as.numeric(p[1]),rate=as.numeric(p[2]))
wilcox.test(dat, rgamma(length(dat),shape=as.numeric(p[1]),
                        rate=as.numeric(p[2])),paired = FALSE)
```

En todos los casos se rechaza la hipótesis nula.

----

## g) Variable **AvgPos**

Esta variable toma valores entre $1$ y $8$, es decir son positivos. Aproximaremos su función de densidad a una Exponencial, Weibull, Uniforme y una Gamma.

```{r}
dat<-data2$AvgPos
summary(dat)
par(mfrow=c(2,2))
plot(dat)
boxplot(dat)
hist(dat)
plot(density(dat))

p<-as.vector(fitdistr(dat,"exponential")$estimate)
fitexp <- fitdist(dat,"exp")
plot(fitexp)
ks.test(dat,"pexp",rate=p)
wilcox.test(dat,rexp(length(dat),rate=p),paired = FALSE)
```

En los gráficos se aprecia que no es muy apropiado aproximarla a esta distribución, y cuya afirmación se confirma con los resultados obtenidos en las pruebas de Kolmogórov y Wilcoxon, ya que en ambas se rechaza la hipótesis nula, es decir existe suficiente evidencia estadística para decir que esta variable no se distribuye Exponencial. Probemos ahora con la distribución Uniforme:

```{r,message=FALSE}
funu<-eUniform(dat)
plot(funu)
ks.test(dat, "punif",min=funu$a,max=funu$b)
wilcox.test(dat,runif(length(dat),min=funu$a,max=funu$b),paired = FALSE)
```

Al igual que con la función de densidad Exponencial sucede lo mismo con esta distribución, este ajuste tampoco es apropiado para los datos, esta conclusión se basa en los resultados obtenidos en las pruebas realizadas, en ambas se rechaza la hipótesis nula. Veamos ahora con una distribución Gamma:

```{r,message=FALSE}
p<-as.vector(fitdistr(dat,"gamma")$estimate)
fitgam<-fitdist(dat,"gamma")
plot(fitgam)
ks.test(dat, "pgamma",shape=p[1],rate=p[2])
wilcox.test(dat, rgamma(length(dat),shape=p[1],rate=p[2]) , paired = FALSE)
```

En estos gráficos se observa un mejor ajuste, pero las pruebas nos indican que se rechaza la hipótesis nula es decir los datos tampoco se distribuyen Gamma. Probemos finalmente con la distribución Weibull.

```{r,message=FALSE}
p<-as.vector(fitdistr(dat,"weibull")$estimate)
fitwei<-fitdist(dat, "weibull")
plot(fitwei)
ks.test(dat, "pweibull",shape=p[1], scale=p[2])
wilcox.test(dat, rweibull(length(dat),shape=p[1], scale=p[2]) , paired = FALSE)
```

Se rechaza la hipótesis nula en todos los casos, pero si refinamos el nivel de significancia podemos aceptarla para el caso de la distribución Weibull.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
p
datos<-data.frame(dat,
                  wei=dweibull(dat,shape=p[1], scale=p[2]))

ggplot(data=datos, mapping = aes(dat))+
geom_histogram(aes(y =..density..),col=I("blue"), alpha=I(.2))+
geom_line(data=datos, aes(dat,wei,colour = I("red")))+
labs(title ="Variable AvgPos", x = "x", y = "f(x)")+
theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

Graficamente esta distribución se aproxima bien a los datos. Hagamos el estudio sacando los outliers y veamos si encontramos una distribución que se aproxime mejor a los datos:

```{r}
y <- na.omit(remove_outliers(dat))
dat<-as.numeric(y)

funu<-eUniform(dat)
#plot(funu)
ks.test(dat, "punif",min=funu$a,max=funu$b)
wilcox.test(dat,runif(length(dat),min=funu$a,max=funu$b),paired = FALSE)

p<-as.vector(fitdistr(dat,"exponential")$estimate)
fitexp <- fitdist(dat,"exp")
#plot(fitexp)
ks.test(dat,"pexp",rate=p)
wilcox.test(dat,rexp(length(dat),rate=p),paired = FALSE)

p<-as.vector(fitdistr(dat,"weibull")$estimate)
fitwei<-fitdist(dat, "weibull")
#plot(fitwei)
ks.test(dat, "pweibull",shape=p[1], scale=p[2])
wilcox.test(dat, rweibull(length(dat),shape=p[1], scale=p[2]) , paired = FALSE)

p<-as.vector(fitdistr(dat,"gamma")$estimate)
fitgam<-fitdist(dat,"gamma")
#plot(fitgam)
ks.test(dat, "pgamma",shape=p[1],rate=p[2])
wilcox.test(dat, rgamma(length(dat),shape=p[1],rate=p[2]) , paired = FALSE)
```

En este caso aceptamos la hipótesis nula en la distribución Weibull y Gamma, mediante el críterio de Akaike seleccionaremos el mejor de ellos:

```{r}
fitwei$aic
fitgam$aic
```

El menor valor nos lo da la distribución Gamma.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
p
datos<-data.frame(y,
                  gam=dgamma(y,shape=p[1],rate=p[2]))

ggplot(data=datos, mapping = aes(y))+
geom_histogram(aes(y =..density..),col=I("blue"), alpha=I(.2))+
geom_line(data=datos, aes(y,gam,colour = I("red")))+
labs(title ="Variable AvgPos", x = "x", y = "f(x)")+
theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

```{r}
boxplot(y)
```

En este caso concluimos que esta variable se distribuye como una Gamma.

----

## h) Variable **Qscore**

Esta variable es discreta y toma valores entre $1$ y $9$, aproximemosla con una Poisson, Binomial negativa, y  Geométrica.

```{r}
dat<-data2$Qscore
summary(dat)
par(mfrow=c(2,2))
plot(dat)
boxplot(dat)
hist(dat)
plot(density(dat))

p<-as.vector(fitdistr(dat,"Poisson")$estimate)
fitpois<-fitdist(dat,"pois")
plot(fitpois)
ks.test(dat,"ppois",lambda=p)
wilcox.test(dat,rpois(length(dat),lambda =p),paired=FALSE)
```

En ambas pruebas se rechaza la hipotesis nula, es decir la variable asociada a `Qscore` no se distribuyen como una Poisson. Ahora probaremos con una distribución Geométrica:

```{r}
p<-as.vector(fitdistr(dat,"geometric")$estimate)
fitgeom <- fitdist(dat, "geom")
plot(fitgeom)
ks.test(dat, "pgeom",prob=p)
wilcox.test(dat , rgeom(length(dat),prob=p) , paired = FALSE)
```

También se rechaza la hipótesis nula en ambas pruebas, probemos con una distribución Binomial negativa:

```{r,warning=FALSE}
p<-as.vector(fitdistr(dat, "negative binomial")$estimate)
fitnbinom <- fitdist(dat, "nbinom")
plot(fitnbinom)
ks.test(dat,"pnbinom",size=p[1],mu=p[2])
wilcox.test(dat,rnbinom(length(dat),size=p[1],mu=p[2]), paired = FALSE)
```

También se rechaza la hipótesis nula en ambos casos.

```{r,echo=FALSE,warning=FALSE,message=FALSE}
p1<-as.vector(fitdistr(dat,"Poisson")$estimate)
p2<-as.vector(fitdistr(dat,"geometric")$estimate)
p3<-as.vector(fitdistr(dat, "negative binomial")$estimate)
p1
p2
p3
datos<-data.frame(dat,
                  pois=dpois(dat,lambda = p1),
                  geom=dgeom(dat,prob=p2 ),
                  nbin=dnbinom(dat,size=p3[1],mu=p3[2]) )

ggplot(data=datos, mapping = aes(dat))+
geom_histogram(aes(y =..density..),col=I("blue"), alpha=I(.2))+
geom_point(data=datos, aes(dat,pois,colour = I("red")))+
geom_point(data=datos, aes(dat,geom,colour = I("green")))+
geom_point(data=datos, aes(dat,nbin,colour = I("black")))+
labs(title ="Variable Qscore", x = "x", y = "f(x)")+
ylim(0,0.7)+
theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

```{r}
boxplot(y)
```

----

## i) Variable **Ventas**

La variable `Ventas` también es una variable discreta, la aproximaremos por las mismas funciones de distribución que se usarón para la variable anterior.

```{r}
dat<-data2$Ventas
summary(dat)
par(mfrow=c(2,2))
plot(dat)
boxplot(dat)
hist(dat)
plot(density(dat))

p<-as.vector(fitdistr(dat,"Poisson")$estimate)
fitpois<-fitdist(dat,"pois")
plot(fitpois)
ks.test(dat,"ppois",lambda=p)
wilcox.test(dat,rpois(length(dat),lambda =p),paired=FALSE)
```

Por los resultados obtenidos la variable `Ventas` no se distribuye como una Poisson. Probemos con una distribución Geométrica:

```{r}
p<-as.vector(fitdistr(dat,"geometric")$estimate)
fitgeom <- fitdist(dat, "geom")
plot(fitgeom)
ks.test(dat, "pgeom",prob=p)
wilcox.test(dat , rgeom(length(dat),prob=p) , paired = FALSE)
```

También se rechaza la hipótesis nula en ambas pruebas, probemos con una distribución Binomial negativa:

```{r,warning=FALSE}
p<-as.vector(fitdistr(dat, "negative binomial")$estimate)
fitnbinom <- fitdist(dat, "nbinom")
plot(fitnbinom)
ks.test(dat,"pnbinom",size=p[1],mu=p[2])
wilcox.test(dat,rnbinom(length(dat),size=p[1],mu=p[2]), paired = FALSE)
```

Se acepta la hipotesis nula en la prueba de Wilcoxon. Por lo tanto podemos decir que estos datos se distribuyen Binomial negativa. 

```{r,message=FALSE,echo=FALSE}
p
datos<-data.frame(dat,
                  nbin=dnbinom(dat,size=p[1],mu=p[2]) )

ggplot(data=datos, mapping = aes(dat))+
geom_histogram(aes(y =..density..),col=I("blue"), alpha=I(.2))+
geom_point(data=datos, aes(dat,nbin,colour = I("red")))+
labs(title ="Variable Ventas", x = "x", y = "f(x)")+
theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

Cuando eliminamos los outliers de estos datos, nos quedamos con datos de un único valor que es cero. Por lo tanto no podemos estimar los parámetros de ninguna de las funciones de densidad.
```{r}
y <- na.omit(remove_outliers(dat))
dat<-y
```

Nos quedaremos con la conclusión de que estos datos se distribuyen Binomial negativa.

----

## j) Variable **Costc**

Esta variable toma valores positivos, ajustemosla a una Exponencial, Gamma y Weibull

```{r}
dat<-data2$Costc
summary(dat)
par(mfrow=c(2,2))
plot(dat)
boxplot(dat)
hist(dat)
plot(density(dat))

p<-eGamma(dat)
plot(p)
ks.test(dat, "pgamma",shape=as.numeric(p[1]),rate=as.numeric(p[2]))
wilcox.test(dat, rgamma(length(dat),shape=as.numeric(p[1]),
                        rate=as.numeric(p[2])) , paired = FALSE)
```

En ambas pruebas se rechaza la hipótesis nula, veamos con la distribución Exponencial:

```{r}
p<-as.vector(fitdistr(dat,"exponential")$estimate)
fitexp <- fitdist(dat,"exp")
plot(fitexp)
ks.test(dat,"pexp",rate=p)
wilcox.test(dat,rexp(length(dat),rate=p),paired = FALSE)
```

También se rechaza la hipótesis nula para ambas pruebas.

```{r,message=FALSE,echo=FALSE}
p1<-eGamma(dat)
p2<-as.vector(fitdistr(dat,"exponential")$estimate)
p1
p2
datos<-data.frame(dat,
                  gam=dgamma(dat,shape=as.numeric(p1[1]),rate=as.numeric(p1[2])),
                  exp=dexp(dat,rate = p2) )

ggplot(data=datos, mapping = aes(dat))+
geom_histogram(aes(y =..density..),col=I("blue"), alpha=I(.2))+
geom_line(data=datos, aes(dat,gam,colour = I("red")))+
geom_line(data=datos, aes(dat,exp,colour = I("green")))+
labs(title ="Variable Costc", x = "x", y = "f(x)")+
theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

Al igual que el caso anterior cuando sacamos los valores atípicos nos encontramos con todos los datos iguales a cero. No tenemos suficientes resultados estadísticos para decidir como se distribuyen los datos.

----

## k) Variable **Convr**

Esta variable aleatoria es continua en el intervalo de $[0,1]$, por lo tanto la vamos aproximar por una Beta, y la distribución Uniforme.

```{r}
dat<-data2$Convr
summary(dat)
par(mfrow=c(2,2))
plot(dat)
boxplot(dat)
hist(dat)
plot(density(dat))

ks.test(dat, "punif",min=0,max=1)
wilcox.test(dat,runif(length(dat),min=0,max=1),paired = FALSE)

funu<-eUniform(dat)
plot(funu)
ks.test(dat, "punif",min=funu$a,max=funu$b)
wilcox.test(dat,runif(length(dat),min=funu$a,max=funu$b),paired = FALSE)

funb<-eBeta(dat)
plot(funb)
ks.test(dat, "pbeta",shape1=funb$shape1,shape2=funb$shape2)
wilcox.test(dat,rbeta(length(dat),shape1=funb$shape1,shape2=funb$shape2),paired = FALSE)
```

Se rechaza la hipótesis nula en los tres casos para ambas funciones. En estos datos la mayoria de ellos es cero, cuando sacamos los outliers nos queda una muestra de puros ceros y con ella no podemos estimar parámetros.


