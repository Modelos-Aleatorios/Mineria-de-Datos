---
title: "outliers"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(readr)
library(ggplot2)
library(fitdistrplus)
library(logspline)
```

#Outliers 

Los outliers son observaciones que se alejan a la gran mayoria de los datos que se tienen. Al aplicar pruebas estadísticas éstos valores tiendes a sesgar los resultados, y llevar a conclusiones no acertadas sobre la realidad de los datos, por ejemplo si se quiere estimar la distribución de los datos puede que estos valores nos alejen de la verdadera distribución dandonos otra distribución no apropiada para explicar mejor los datos.

Para encontrar estos datos atípicos o poco comunes, se usan pruebas prueba de shapiro, dixon o chi-cuadrado, el problema de estas pruebas es que se tiene que asumir normalidad de los datos, en nuestro caso no parece cumplirse con esta suposición, veamos si es así:

```{r, echo=TRUE}
data <- read_delim("../data/ROW.csv", 
    ";", escape_double = FALSE, locale = locale(decimal_mark = ",", 
    grouping_mark = "."), trim_ws = TRUE)
#Eliminada la variable Allconv
data<-data[,-13]
data
#Renombrar las variables
colnames(data)<-c("Adgroup","MaxCPC","Clicks","Impr",
                    "CTR","AvgCPC","Costo","AvgPos","Qscore",
                    "Ventas","Costc","Convr")
data_original<-data
#Eliminar los datos que tienen 0 impresiones
data<-data[data$Impr!=0,]
```

Primero comprobaremos si cada variable se distribuye normalmente para aplicar estas pruebas,
```{r}
#Variable MaxCPC
shapiro.test(data$MaxCPC)
#Variable Clicks
shapiro.test(data$Clicks)
#Variable Impr
shapiro.test(data$Impr)
#Variable CTR
shapiro.test(data$CTR)
#Variable AvgCPC
shapiro.test(data$AvgCPC)
#Variable Costo
shapiro.test(data$Costo)
#Variable AvgPos
shapiro.test(data$AvgPos)
#Variable Qscore
shapiro.test(data$Qscore)
#Variable Ventas
shapiro.test(data$Ventas)
#Variable Costc
shapiro.test(data$Costc)
#Variable Convr
shapiro.test(data$Convr)
```

En todos los caso el p-valor es menor que el nivel de significancia $\alpha=0.05$, por lo tanto en todos los casos se rechaza la hipótesis nula, es decir, ninguna de las variables se distribuye normalmente. 

Existe también el diagrama de cajas que nos da la distribución de los datos, cálcula la mediana de los datos y los quantiles 1, 2 y 3 los valores que queden fuera de el tercer cuantil y primer cuantil son considerados datos atípicos, veamos que obtenemos en todas las variables.

```{r}
#Función para eliminar los outliers segun el cuantil 0.25 y 0.75
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}
```

A continuación veamos como se distribuye cada variable con los datos completos y sin los outliers, se aplicara una prueba de Kolmogorov smirnov y un test de wilconxon para comprobar que los datos se distribuyen como la función que encontremos con el comando `fitdistr()`

a) Variable **MaxCPC**:

```{r}
dat<-data$MaxCPC
#summary(dat)

#Aproximarlo con una Exponencial
fitexp<-fitdistr(dat, "exponential")
fitexp <- fitdist(dat, "exp")
plot(fitexp)

#Prueba de Kolmogorov-smirnov
ks.test(dat, "pexp",rate=1/2.8542086)
#Test de Wilconxo
wilcox.test(dat , rexp(length(dat),rate = 1/2.8542086) , paired = FALSE)
```

En ambas pruebas se rechaza la hipótesis nula por lo tanto los datos no se distribuyen exponencial. Veamos que pasa con la aproximación de una función de densidad Gamma:

```{r}
#Aproximarlo con una Gamma
fitdistr(dat, "gamma")
fitgam<-fitdist(dat, "gamma")
plot(fitgam)

ks.test(dat, "pgamma",shape=1.55447879, rate=4.43680739)
wilcox.test(dat, rgamma(length(dat),shape=1.55447879, rate=4.43680739) , paired = FALSE)
```

Se rechaza la prueba de kolmogov, pero se acepta la prueba de Wilcoxon, si nos quedamos con este último resultado podemos decir que los datos asociados a esta variable se distribuyen Gamma. Veamos finalmente que pasa si la aproximamos con una densidad Weibull:

```{r}
#Aproximarlo con una Weibull
fitdistr(dat, "weibull")
fitwei<-fitdist(dat, "weibull")
plot(fitwei)

ks.test(dat, "pweibull",shape=1.21871055, scale=0.37615830)
wilcox.test(dat, rweibull(length(dat),shape=1.21871055, scale=0.37615830) , paired = FALSE)
```

Sucede lo mismo que la aproximación a una Gamma, se rechaza la hipótesis nula de Komogorov pero se acepta en el caso de la prueba de Wilconxon. Entonces podríamos decir que estos datos se distribuyen Weibull o Gamma bajo la prueba de Wilcoxon.

```{r}
datos<-data.frame(dat,
       gam=dgamma(dat,shape=1.55447879, rate=4.43680739),
       wei=dweibull(dat,shape=1.21871055, scale=0.37615830))

ggplot(data=datos, mapping = aes(dat))+geom_histogram(aes(y =..density..),col=I("blue"), alpha=I(.2))+
  geom_line(data=datos, aes(dat,gam,colour = I("red")))+
  geom_line(data=datos, aes(dat,wei,colour = I("green")))+
  labs(title ="Variable MaxCPC", x = "x", y = "f(x)")+
  theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

Veamos que pasa con los datos sin los outliers
```{r}
#Removiendo los outliers, para comparar distribuciones
y <- na.omit(remove_outliers(dat))

#Aproximarlo con una Exponencial
fitdistr(y, "exponential")
ks.test(y, "pexp",rate=1/3.5892323)
wilcox.test(dat , rexp(length(y),rate = 1/3.5892323) , paired = FALSE)

#Aproximarlo con una Gamma
fitdistr(y, "gamma")
ks.test(y, "pgamma",shape=2.0778448, rate=7.4578617)
wilcox.test(y, rgamma(length(y),shape=2.0778448, rate=7.4578617) , paired = FALSE)

#Aproximarlo con una Weibull
fitdistr(y, "weibull")
ks.test(y, "pweibull",shape=1.566819499, scale=0.310277207)
wilcox.test(y, rweibull(length(y),shape=1.566819499, scale=0.310277207) , paired = FALSE)
```

Encontramos los mismos resultados que para los datos completos, entonces podemos decir que los datos se distribuyen Gamma o Weibull.

```{r}
datos<-data.frame(y,
       gam=dgamma(y,shape=2.0778448, rate=7.4578617),
       wei=dweibull(y,shape=1.566819499, scale=0.310277207))

ggplot(data=datos, mapping = aes(y))+geom_histogram(aes(y =..density..),fill=I("pink"),col=I("pink"), alpha=I(.2))+
  geom_line(data=datos, aes(y,gam,colour = I("red")))+
  geom_line(data=datos, aes(y,wei,colour = I("green")))+
  labs(title ="Variable MaxCPC", x = "x", y = "f(x)")+
  theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

b) Variable **Click**

```{r}
dat<-data$Clicks
#summary(dat)

#Aproximarlo con un Poisson
fitdistr(dat, "Poisson")
fitpois <- fitdist(dat, "pois")
plot(fitpois)

ks.test(dat, "ppois",lambda=12.1889055)
wilcox.test(dat,rpois(length(dat),lambda = 12.1889055),paired = FALSE)
```

En ambas pruebas se rechaza la hipotesis nula, es decir la variable asociada a `CLicks` no se distribuyen como una Poisoon. Ahora probaremos con una distribución Geométrica:

```{r}
fitdistr(dat, "geometric")
fitgeom <- fitdist(dat, "geom")
plot(fitgeom)

ks.test(dat, "pgeom",prob=0.07582130)
wilcox.test(dat , rgeom(length(dat),prob=0.07582130) , paired = FALSE)
```

También se rechaza la hipótesis nula en ambas pruebas, probemos con una distribución binomial negativa:
```{r}
fitdistr(dat, "negative binomial")
fitnbinom <- fitdist(dat, "nbinom")
plot(fitnbinom)

ks.test(dat,"pnbinom",size=0.26438439,mu=12.18890560)
wilcox.test(dat,rnbinom(length(dat),size=0.26438439,mu=12.18890560), paired = FALSE)
```

En este caso se rechaza la prueba de Kolmogorov, pero se acepta la hipótesis nula de la prueba de Wilconxo, por lo tanto podemos decir que la variable aleatoria asociada a los `CLicks` se distribuye binomial negativa.

```{r}
datos<-data.frame(dat,nbinom=dnbinom(dat,size=0.26438439,mu=12.18890560))

ggplot(data=datos, mapping = aes(dat,nbinom, colour = I("blue")))+geom_point()+
geom_histogram(mapping = aes(dat,y =..density..,col = I("blue")),fill=I("blue"),alpha=I(.1))+
  labs(title ="Variable Clicks", x = "x", y = "f(x)")+
  theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

Veamos que pasa con los datos sin los outliers
```{r}
#Removiendo los outliers, para comparar distribuciones
y <- na.omit(remove_outliers(dat))

#Aproximarlo con una Poisson
fitdistr(y, "Poisson")
ks.test(y, "ppois",lambda=3.52261307)
wilcox.test(y,rpois(length(y),lambda = 3.52261307),paired = FALSE)

#Aproximarlo por una Geométrica
fitdistr(y, "geometric")
ks.test(y, "pgeom",prob=0.221111111)
wilcox.test(y,rgeom(length(y),prob=0.221111111) , paired = FALSE)

#Aproximarlo por una Binomial Negativa
fitdistr(y, "negative binomial")
ks.test(y,"pnbinom",size=0.52383098,mu=3.52276725)
wilcox.test(y,rnbinom(length(y),size=0.52383098,mu=3.52276725), paired = FALSE)
```

Por lo observado solo se acepta la hipótesis nula en la prueba de Wilconxon para la comparación de la binomial negativa para los datos sin los outliers. Recordemos que este fue el mismo resultado para los datos completos.

```{r}
datos<-data.frame(y, nbinom=dnbinom(y,size = 0.52383098, mu = 3.52276725))

ggplot(data=datos, mapping = aes(y,nbinom, colour = I("blue")))+geom_point()+
  geom_histogram(mapping = aes(y,y =..density..,col = I("blue")),fill=I("blue"),alpha=I(.1))+
  labs(title ="Variable Clicks", x = "x", y = "f(x)")+
  theme(plot.title = element_text(size = rel(1.3),hjust = 0.5))
```

c) Variable **Impr**

```{r}
dat<-data$Impr
#summary(dat)

#Aproximarlo con un Poisson
fitdistr(dat, "Poisson")
fitpois <- fitdist(dat, "pois")
plot(fitpois)

ks.test(dat, "ppois",lambda=698.044978)
wilcox.test(dat,rpois(length(dat),lambda = 698.044978),paired = FALSE)
```

En ambas pruebas se rechaza la hipotesis nula, es decir la variable asociada a `Impr` no se distribuyen como una Poisoon. Ahora probaremos con una distribución Geométrica:

```{r}
fitdistr(dat, "geometric")
ks.test(dat, "pgeom",prob=1.430523e-03)
wilcox.test(dat , rgeom(length(dat),prob=1.430523e-03) , paired = FALSE)
```

También se rechaza la hipótesis nula en ambas pruebas, probemos con una distribución binomial negativa:

```{r}
fitdistr(dat, "negative binomial")
fitnbinom <- fitdist(dat, "nbinom")
plot(fitnbinom)

ks.test(dat,"pnbinom",size=0.2562254,mu=698.0449775)
wilcox.test(dat,rnbinom(length(dat),size=0.2562254,mu=698.0449775), paired = FALSE)
```
