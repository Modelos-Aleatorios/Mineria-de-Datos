---
title: "Proyecto Tópicos I - Cluster"
author: "Alexander A. Ramírez M. (alexanderramirez.me) y Daysi Febles (daysilorenafeblesr@gmail.com)"
output:
  pdf_document: 
    toc: yes
    toc_depth: 5
    fig_width: 6
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
```

----

# Cluster

Los cluster son la agrupación de grupos homogéneos de un conjunto de variables tomadas de un grupo de individuos, dentro de cada grupos homogéneos los individuos tienen que ser similares entre ellos. 

Aplicaremos esta técnica a nuestros datos.

Primeramente prepararemos los datos:

```{r,echo=FALSE,warning=FALSE,message=FALSE}
data <- read_delim("../data/ROW.csv", 
    ";", escape_double = FALSE, locale = locale(decimal_mark = ",", 
    grouping_mark = "."), trim_ws = TRUE)
#Eliminada la variable Allconv
data<-data[,-13]
#Renombrar las variables
colnames(data)<-c("Adgroup","MaxCPC","Clicks","Impr",
                    "CTR","AvgCPC","Costo","AvgPos","Qscore",
                    "Ventas","Costc","Convr")
data_original<-data

data<-data_original[data$Impr!=0,]

#Eliminar la variable Adgroup
data1<-data[,-1]
```

Luego de tener nuestros datos, vamos a estandarizar todas las variables, es decir restandole la respectiva media y dividiendola entra la varianza, esto se hace con el comando `scale()`
```{r}
mydata <- scale(data1) # Estandarizar las variables
```

Por el análisis de componente principales decidimos que la cantidad de cluster que vamos a estimar son dos
```{r}
# K-Means Cluster Analysis
fit <- kmeans(mydata, 3) # 2 cluster solution
# get cluster means 
aggregate(mydata,by=list(fit$cluster),FUN=mean)
# append cluster assignment
mydata1 <- data.frame(data$Adgroup, mydata, fit$cluster)

cluster1<-subset(mydata1,mydata1$fit.cluster==1)
table(as.vector(cluster1$data.Adgroup))

cluster2<-subset(mydata1,mydata1$fit.cluster==2)
table(as.vector(cluster2$data.Adgroup))

cluster3<-subset(mydata1,mydata1$fit.cluster==3)
table(as.vector(cluster3$data.Adgroup))
```

```{r}
# Ward Hierarchical Clustering
d <- dist(mydata, method = "euclidean") # distance matrix
fit <- hclust(d, method="ward") 
plot(fit) # display dendogram
groups <- cutree(fit, k=3) # cut tree into 5 clusters
# draw dendogram with red borders around the 5 clusters 
rect.hclust(fit, k=, border="red")
```


```{r}
library(pvclust)
fit <- pvclust(mydata,nboot = 100)
plot(fit) # dendogram with p values
# add rectangles around groups highly supported by the data
pvrect(fit, alpha=.95)
pvpick(fit, alpha=0.95)
```